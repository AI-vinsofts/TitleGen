{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-z\n",
    "# https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb#scrollTo=UK-hmKjYVoll\n",
    "# https://colab.research.google.com/drive/11ujHkZmwOY1oj55Dad2C88h9WkIshGHs?authuser=1#scrollTo=tGMHNyz6RpBD\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time \n",
    "import string\n",
    "from underthesea import sent_tokenize\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[' ', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', 'Á', 'Â', 'Ô', '×', 'Ù', 'Ú', 'Ý', 'à', 'á', 'â', 'ã', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ù', 'ú', 'ý', 'ă', 'Đ', 'đ', 'ĩ', 'ũ', 'ơ', 'Ư', 'ư', 'ạ', 'Ả', 'ả', 'Ấ', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'Ắ', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ẹ', 'ẻ', 'ẽ', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ỉ', 'ị', 'ọ', 'ỏ', 'Ố', 'ố', 'ồ', 'Ổ', 'ổ', 'ỗ', 'ộ', 'ớ', 'ờ', 'Ở', 'ở', 'ỡ', 'ợ', 'ụ', 'Ủ', 'ủ', 'Ứ', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'ỳ', 'ỵ', 'ỷ', 'ỹ', '\\u200b', '–', '‘', '’']\n"
     ]
    }
   ],
   "source": [
    "string.punctuation = string.punctuation + '\\“”…'\n",
    "\n",
    "def preprocessing_data(text):\n",
    "    translator = str.maketrans('', '', string.punctuation.replace(\".\",\"\").replace(\"-\", \"\").replace(\"/\", \"\").replace(\":\", \"\")) # remove punctuation\n",
    "    text = text.translate(translator)\n",
    "    text = \" \".join(text.split()) \n",
    "    # text = text.lower()\n",
    "    text = str(text)\n",
    "    return text\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = 159 # = len(vocab)\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "with open(\"charvocab.txt\", \"r\") as fp:\n",
    "    vocab = json.load(fp)\n",
    "print(vocab)\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (1, None, 128)            20352     \n_________________________________________________________________\nlstm (LSTM)                  (1, None, 1024)           4722688   \n_________________________________________________________________\ndense (Dense)                (1, None, 159)            162975    \n=================================================================\nTotal params: 4,906,015\nTrainable params: 4,906,015\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "# model = build_model(vocab_size=len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
    "\n",
    "model1.load_weights(\"ckpt_100\")\n",
    "model1.build(tf.TensorShape([32, None]))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nYamaha Exciter 150 theo phong cách môtô đua này có giá 76 triệu đồng cho bản ABS cá tính. Nguyên bản chiếc Honda Cross Cub 110 sẽ có mặt tại đại lý từ ngày 20/10.Tại thị trường Việt Nam BMW Motorrad Việt Nam còn phân phối phiên bản R của Latte là 790 mm trong khi các mẫu xe tay ga được bán chính hãng tại Việt Nam.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "    num_generate = 500\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "    temperature = 0.1\n",
    " \n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        if idx2char[predicted_id] == \". \":\n",
    "            break\n",
    "        else:\n",
    "            text_generated.append(idx2char[predicted_id])\n",
    "    # return the first sentence\n",
    "    return sent_tokenize(start_string + ''.join(text_generated))[:2]\n",
    "\n",
    "\n",
    "# INPUT TEXT HERE\n",
    "input = \"Yamaha Exciter\"\n",
    "headline = generate_text(model1, start_string = input)\n",
    "# print(headline[:2])\n",
    "sentence = str(headline[0]) + \" \" +str(headline[1])\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}