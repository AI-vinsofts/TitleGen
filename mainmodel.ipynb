{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-z\n",
    "# https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb#scrollTo=UK-hmKjYVoll\n",
    "# https://colab.research.google.com/drive/11ujHkZmwOY1oj55Dad2C88h9WkIshGHs?authuser=1#scrollTo=tGMHNyz6RpBD\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time \n",
    "import string\n",
    "from underthesea import sent_tokenize\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[' ', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', 'Á', 'Â', 'Ô', '×', 'Ù', 'Ú', 'Ý', 'à', 'á', 'â', 'ã', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ù', 'ú', 'ý', 'ă', 'Đ', 'đ', 'ĩ', 'ũ', 'ơ', 'Ư', 'ư', 'ạ', 'Ả', 'ả', 'Ấ', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'Ắ', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ẹ', 'ẻ', 'ẽ', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ỉ', 'ị', 'ọ', 'ỏ', 'Ố', 'ố', 'ồ', 'Ổ', 'ổ', 'ỗ', 'ộ', 'ớ', 'ờ', 'Ở', 'ở', 'ỡ', 'ợ', 'ụ', 'Ủ', 'ủ', 'Ứ', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'ỳ', 'ỵ', 'ỷ', 'ỹ', '\\u200b', '–', '‘', '’']\n"
     ]
    }
   ],
   "source": [
    "string.punctuation = string.punctuation + '\\“”…'\n",
    "\n",
    "def preprocessing_data(text):\n",
    "    translator = str.maketrans('', '', string.punctuation.replace(\".\",\"\").replace(\"-\", \"\").replace(\"/\", \"\").replace(\":\", \"\")) # remove punctuation\n",
    "    text = text.translate(translator)\n",
    "    text = \" \".join(text.split()) \n",
    "    # text = text.lower()\n",
    "    text = str(text)\n",
    "    return text\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = 159 # = len(vocab)\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "with open(\"charvocab.txt\", \"r\") as fp:\n",
    "    vocab = json.load(fp)\n",
    "print(vocab)\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (1, None, 128)            20352     \n_________________________________________________________________\nlstm (LSTM)                  (1, None, 1024)           4722688   \n_________________________________________________________________\ndense (Dense)                (1, None, 159)            162975    \n=================================================================\nTotal params: 4,906,015\nTrainable params: 4,906,015\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "# model = build_model(vocab_size=len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
    "\n",
    "model1.load_weights(\"ckpt_100\")\n",
    "model1.build(tf.TensorShape([32, None]))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nYamaha Exciter 150 theo phong cách môtô đua này có giá 76 triệu đồng cho bản ABS cá tính. Nguyên bản chiếc Honda Cross Cub 110 sẽ có mặt tại đại lý từ ngày 20/10.Tại thị trường Việt Nam BMW Motorrad Việt Nam còn phân phối phiên bản R của Latte là 790 mm trong khi các mẫu xe tay ga được bán chính hãng tại Việt Nam.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "    num_generate = 500\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "    temperature = 0.1\n",
    " \n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        if idx2char[predicted_id] == \". \":\n",
    "            break\n",
    "        else:\n",
    "            text_generated.append(idx2char[predicted_id])\n",
    "    # return the first sentence\n",
    "    return sent_tokenize(start_string + ''.join(text_generated))[:2]\n",
    "\n",
    "headline = generate_text(model1, start_string = \"Yamaha Exciter \")\n",
    "# print(headline[:2])\n",
    "sentence = str(headline[0]) + \" \" +str(headline[1])\n",
    "print()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = open(\"Datasetmrhoang/Xe cộ/article_100.txt\", \"r\", encoding='utf-8').read()\n",
    "\n",
    "# text = preprocessing_data(text)\n",
    "# text = text + text\n",
    "\n",
    "# text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# # Show how the first 13 characters from the text are mapped to integers\n",
    "# print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n",
    "\n",
    "# # The maximum length sentence you want for a single input in characters\n",
    "# seq_length = 100\n",
    "# examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# # Create training examples / targets\n",
    "# char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# for i in char_dataset.take(5):\n",
    "#     print(idx2char[i.numpy()])\n",
    "\n",
    "# sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "# for item in sequences.take(5):\n",
    "#     print(repr(''.join(idx2char[item.numpy()])))\n",
    "\n",
    "# def split_input_target(chunk):\n",
    "#     input_text = chunk[:-1]\n",
    "#     target_text = chunk[1:]\n",
    "#     return input_text, target_text\n",
    "\n",
    "# dataset = sequences.map(split_input_target)\n",
    "\n",
    "\n",
    "# # Batch size\n",
    "# BATCH_SIZE = 128\n",
    "\n",
    "# # Buffer size to shuffle the dataset\n",
    "# # (TF data is designed to work with possibly infinite sequences,\n",
    "# # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# # it maintains a buffer in which it shuffles elements).\n",
    "# BUFFER_SIZE = 10000\n",
    "\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "# # The embedding dimension\n",
    "# embedding_dim = 256\n",
    "\n",
    "# # Number of RNN units\n",
    "# rnn_units = 1024\n",
    "\n",
    "\n",
    "# model2 = build_model(vocab_size=len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
    "\n",
    "# for input_example_batch, target_example_batch in dataset.take(1):\n",
    "#     example_batch_predictions = model2(input_example_batch)\n",
    "#     print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "# model2.summary()\n",
    "\n",
    "# sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "# sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "# print(sampled_indices)\n",
    "\n",
    "# def loss(labels, logits):\n",
    "#     return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "# model2.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# # Directory where the checkpoints will be saved\n",
    "# checkpoint_dir = './training_checkpoints_art100'\n",
    "# # Name of the checkpoint files\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_prefix,\n",
    "#     save_weights_only=True)\n",
    "\n",
    "# EPOCHS = 500\n",
    "# start_time = time.time()\n",
    "\n",
    "# history = model2.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback], batch_size=64)\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(elapsed_time)\n",
    "\n",
    "# model3 = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "# model3.load_weights(\"./training_checkpoints/ckpt_494\")\n",
    "# model3.build(tf.TensorShape([32, None]))\n",
    "# model3.summary()\n",
    "\n",
    "# headline = generate_text(model3, start_string = \"VinFast \")\n",
    "# print(headline)"
   ]
  }
 ]
}